
架构师之路精选
====================

#### 互联网架构如何实现“高并发”
* 什么是高并发：通过设计保证系统能够同时并行处理很多请求
  - 相关指标：响应时间、吞吐量、QPS
* 如何提升并发能力
  - 垂直扩展（Scale Up）：业务发展前期建议使用（效果最快）
    - 提升单机处理能力：
      - 增强硬件：增加CPU数、升级网卡、升级硬盘、内存
      - 增强架构性能：缓存减少IO，异步增加吞吐量、无锁数据结构减少响应时间
  - 水平扩展(Scale Out)：终极方案
    - 常见互联网分层架构：客户端层、反向代理层、站点应用层、服务层、数据-缓存层、数据-数据库层
    - 分层水平扩展实践
      - 反向代理层：DNS轮询
      - 站点层：通过nginx实现，设置多个web后端
      - 服务层：通过服务连接池实现
      - 数据层：垂直拆分、水平拆分（按范围拆、按哈希拆）
        
#### TCP接入层的负载均衡、高可用、扩展性架构
* TCP是有状态的连接，一个client发起的请求必须落在同一台tcp-server上，如何做负载均衡，如何保证水平扩展呢
  - 单机法tcp-server：
    - 优点：请求一致性
    - 缺点：无法保证高可用
  - 集群法tcp-server，客户端实现负载均衡（配置多个外网IP，随机选择其一）
    - 优点：可保证高可用
    - 缺点：多一次DNS访问，难以预防DNS劫持
      - 改进：将IP配置在客户端（IP直通车）
        - 缺点：扩展性差，如IP变化、新增都需要更新客户端
  - 服务端实施负载均衡（根本上解决问题）
    - 增加一个http接口，将客户端的“IP配置”与“均衡策略”放到服务端，client每次访问tcp-server前，先调用一个新增的get-tcp-ip接口，这个http接口只返回一个tcp-server的IP
      - 优点：解决扩展性问题
      - 缺点：接口不知道tcp-server集群中每台服务器是否可用
  - tcp-server状态上报
    - 优点：解决上面问题
    - 缺点：反向依赖
  - tcp-server状态拉取
    - 优点：解决上面问题
    
#### 跨公网调用的大坑与架构优化方案
* 缘起
  - 业务需要跨公网调用一个第三方服务提供的接口，往往抽象出一个服务
  - 调用流程：1）业务方调用内部服务，2）内部服务跨公网调用三方，3）三方返回结果， 4）内部服务返回结果给业务方
    - 存在问题：某个调三方接口超时，会导致所有接口不可用
* 异步代理法
  - 异步代理服务定期跨公网调用三方服务，刷新本地数据
  - 优点：解决接口超时问题
  - 缺点：不是最新数据
* 第三方接口备份与切换法
  - 超时后调用第二个备份三方服务，直到超时服务恢复
  - 优点：解决超时问题
  - 缺点：不是所有三方有备用接口
* 异步调用法
  - 本地写成功就算成功，异步向第三方同步数据    
    
#### DNS在架构设计中的巧用
* 反向代理水平扩展
  - 配置多个nginx的外网IP，轮询返回
* web-server负载均衡（很少用）
  - 优点：第三方实施，不用改动架构；减少一层网络请求
  - 缺点：只具备解析功能，不保证IP的可用性（nginx做反向代理有保活探测机制，能自动迁移流量）；web-server扩容时，dns个性生效时间长
* 用户就近访问：智能DNS
  
#### 到底什么时候该使用MQ？
* 消息总线（Message Queue），后文称MQ，是一种跨进程的通信机制，用于上下游传递消息。
* 什么时候不使用MQ：上游实时关注执行结果
  - 缺点：1）系统更复杂，2）消息传递路径更长，延时增加，3）消息可靠性和重复性互相矛盾，4）上游无法知道下游执行结果
* 什么时候使用MQ
  - 场景一：数据驱动的任务依赖
    - 常规作法：cron定时执行
      - 缺点：1）超时问题严重；2）执行时间很长，预留大量buffer；3）依赖关系容易出错；4）调整一个任务影响很大
    - 优化方案：采用MQ解耦
      - 优点：1）不需要预留buffer；2）方便处理依赖；3）调整任务，不影响下游执行时间
  - 场景二：上游不关心执行结果
    - 优点：1）上游执行时间短；2）上下游逻辑解耦；3）新增下游，不用改上游
  - 场景三：上游关心执行结果，但执行时间很长
    - 三方服务，回调网关通知MQ  
  
#### 消息总线能否实现消息必达？
* MQ要想尽量消息必达，架构上有两个核心设计点：1）消息落地；2）消息超时、重传、确认

#### 消息总线真的能保证幂等？
* MQ-Client发消息给MQ-Server
  - 对每条消息，S内部生成inner-msg-id，用于去重，其特征：1）全局唯一；2）MQ生成，无业务无关
* MQ-Server发消息给MQ-Client
  - 生成biz-id，用于去重，其特征：1）同一业务场景全局唯一；2）由业务发送方生成，对MQ透明；3）业务消费方负责判重，保证幂等
  - 常见业务ID：支付ID，帖子ID
* 幂等性，不仅对MQ有要求，对业务上下游也有要求。
  
#### MQ，如何做到消息延时
* 常见方案：启动一个cron定时任务，每小时执行一次
  - 缺点：1）轮询效率低；2）重复计算；3）时效性不好，误差大；
* 高效延时消息设计与实现
  - 环形队列：例如一个3600个slot的队列
  - 任务集合：每个slot是一个Set<Task>
  - timer每秒移动一格，Cycle-Num为0则处理任务，否则减1
  - 优点：1）无需轮询；2）一个任务只执行一次；3）时效性好，精确到秒  
  
#### MQ如何快速实现流量削峰填谷
* 常见方案：1）上游队列缓冲，限速发送；2）下游队列缓冲，限速执行
* MQ缓冲方式：
  - 由MQ-Server推模式，改为MQ-Client拉模式
  - 如果MQ-Server堆积消息过多，Client要改进为批量写，提高吞吐量  
  

#### 消息安全传输中的技术点
* 初级阶段：在网络上传递明文，非常危险
* 传输密文
  - 客户端和服务端约定好加密算法和密钥
  - 缺点：客户端代码不安全
* 服务端为每个用户生成密钥
  - 客户端和服务端提前约定好加密算法，在传递消息前，先协商密钥
* 客户端确定密钥，密钥不再传输（弱安全）
  - 特点：
    - 使用“具备用户特性的东西”作为加密密钥，例如：用户密码的散列值
    - 一人一密，每个人的密钥不同
* 一次一密，密钥协商
  - 过程
    - 服务端随机生成公私钥对(公钥pk1，私钥pk2)，并将公钥pk1传给客户端
    - 客户端随机生成公私钥对(公钥pk11，私钥pk22)，并将公钥pk22，通过pk1加密，传给服务端
    - 服务端随机生成对称加密密钥key=X，用pk11加密，传给客户端
* 总结
  - 1）网络上传递的数据是不安全的；2）客户端的代码是不安全的；3）客户端内存是安全的；
  
#### 互联网分层架构的本质
* 典型互联网分层架构：1）客户端；2）应用层；3）数据缓存层；4）数据库层
  - 服务化：2，3之间添加一个服务层
* 同层次MVC分层
* 互联网分层架构，是一个数据移动，处理，呈现的过程，其中数据移动是整个过程的核心
* 数据移动的两个重点：1）数据传输的格式；2）数据在各层次的形态
  - 数据传输的格式[协议]
    - service与db/cache之间，二进制协议/文本协议
    - web-server与service之间，RPC的二进制协议
    - client和web-server之间，http协议
  - 数据在各层次的形态
    - db层，数据是以“行”为单位存在的row(uid, name, age)
    - cache层，数据是以kv的形式存在的kv(uid -> User)
    - service层，会把row或者kv转化为对程序友好的User对象
    - web-server层，会把对程序友好的User对象转化为对http友好的json对象
    - client层：最终端上拿到的是json对象
* 核心原则与方法：封装（下游屏蔽数据的获取细节）和复用（上游高效地获取和处理数据） 
  
#### 互联网架构为什么要做服务化？
* 为什么要服务化
  - 架构痛点一：代码到处拷贝 - 各个业务线都是自己通过DAO写SQL访问库
  - 架构痛点二：复杂性扩散 - 各个业务线都需要关注缓存的引入导致的复杂性
  - 架构痛点三：库的复用与耦合
  - 架构痛点四：SQL质量得不到保障，业务相互影响
  - 架构痛点五：疯狂的DB耦合  
  
#### 互联网分层架构之-DAO与服务化
* 什么时候进行DAO层的抽象
  - 当手写代码从DB中获取数据，成为通用痛点的时候，就应该抽象出DAO层，简化数据获取过程，提高数据获取效率，向上游屏蔽底层的复杂性
* 什么时候进行数据服务层的抽象
  - 当业务越来越复杂，垂直拆分的系统越来越多，数据库实施了水平切分，数据层实施了缓存加速之后，底层数据获取复杂性成为通用痛点的时候，就应该抽象出数据服务层，简化数据获取过程，提高数据获取效率，向上游屏蔽底层的复杂性
  - 服务化的缺点：1）请求处理时间增加；2）运维复杂；3）定时问题更麻烦  
  
#### 业务层是否也需要服务化
* 业务层是否需要服务化
  - 是否需要抽象通用业务服务，和业务复杂性，以及业务发展阶段有关，不可一概而论
* 什么时候服务化
  - 业务越来越复杂，垂直拆分的系统越来越多，基础数据服务越来越多，底层数据获取复杂性成为通用痛点的时候，就应该抽象出通用业务服务，简化数据获取过程，提高数据获取效率，向上游屏蔽底层的复杂性  
  
#### 为什么要引入数据库中间件
* 数据量增大，性能会降低，开始对DB进行切分，原来SQL支持的功能，需要base-service层来进行特殊处理
  - 有些数据需要路由到特定的水平切分库
  - 有些数据不确定落在哪一个水平切分库，就需要访问所有库
  - 有些数据需要访问全局的库，拿到数据的全局视野，到service层进行额外处理
* 带来的问题：1）base-service层的复杂度提高了；2）数据获取效率降低了
* 解决方案，引入中间件db-proxy
* 结论：当数据库水平切分，base-service层获取db数据过于复杂，成为通用痛点的时候，就应该抽象出数据库中间件，简化数据获取过程，提高数据获取效率，向上游屏蔽底层的复杂性。  
  
#### APP分层架构设计随想
* 问题：大量复制代码，无法重用
  - 原因：1）早期业务压力大，没做规划；2）后期代码不敢动，怕出问题；3）一个人图省事；
* 分层：
  - 1）View-展现层；
  - 2）Controller-服务层，实现业务逻辑，提供封装和复用；
  - 3）Model-数据层，
    - 同步获取：通过文件，内存，本地数据库获取
    - 异步获取：从server获取，往往通过回调实现  
  
#### 架构解耦
* 耦合，是架构中，本来不相干的代码、模块、服务、系统因为某些原因联系在一起，各自独立性差，影响则相互影响，变动则相互变动的一种架构状态。
  - 如何发现：明明不应该联动，却要被动配合，就可能有潜在的耦合
* IP耦合
  - 如何解决：使用内网域名替代内网IP
* 公共库耦合
  - 如何解决：通常是方案二，三同时使用
    - 方案一：代码拷贝一份，但缺点很多
    - 方案二：垂直拆分，拆出业务个性化代码
    - 方案三：服务化，将通用代码拆到下层
* 数据库耦合
  - 公用业务表和独立业务数据表，必须在同一个数据库实例里，形成耦合
  - 如何解决
    - 第一步：公共数据访问下沉服务化 - 业务层只能通过服务RPC接口访问数据；任何上游不允许跨过服务访问底层的user库
    - 第二步：垂直拆分，个性化数据访问上浮 - 服务化+垂直拆分后，变成两次访问，一次获取业务数据，一次获取通用数据
  - 优点：数据库解耦
  - 缺点：业务代码复杂，多次访问，SQL中的逻辑计算迁移到业务代码中
* MQ，互联网架构解耦神器
  - 如果事件发出方关注下游执行执行结果，用RPC；不关心订阅方的执行结果，应该用MQ
* 配置中心，互联网架构解耦利器  
  
#### session一致性架构设计实践
* session一致性问题：多台web-server来保证高可用时，每次http短连接请求就不一定能路由到正确的session了
* session同步法：多个web-server之间相互同步session
  - 优点：web-server支持的功能，应用程序不需要修改代码
  - 缺点：1）session的同步需要数据传输，占内网带宽，有时延；2）所有web-server都包含所有session数据，数据量受内存限制，无法水平扩展；3）有更多web-server时要歇菜
* 客户端存储法：将session存储到浏览器cookie中，此方案不常用
  - 优点：服务端不需要存储
  - 缺点：1）占外网带宽；2）存在泄漏、篡改、窃取等安全隐患；3）session存储的数据大小受cookie限制
* 反向代理hash一致性
  - 方案一：四层代理hash - 使用用户ip来做hash，以保证同一个ip的请求落在同一个web-server上
  - 方案二：七层代理hash - 使用业务属性，以保证同一个用户的请求落在同一个web-server上
    - 优点：1）只改nginx配置，不改代码；2）负载均衡；3）支持水平扩展
    - 缺点：1）如果web-server重启，一部分session会丢失；2）如果web-server水平扩展，rehash后session重新分布，也会有一部分用户路由不到正确的session
* 后端统一存储：将session存储在web-server后端的存储层，数据库或者缓存（推荐）
  - 优点：1）没有安全隐患；2）可以水平扩展；3）web-server重启或者扩容都不会有session丢失
  - 缺点：增加了一次网络调用，并且需要修改应用代码
  
  
#### MySQL双主一致性架构优化
* 在一个MySQL数据库集群中可以设置两个主库，并设置双向同步，以冗余写库的方式来保证写库的高可用
* 数据冗余会引发数据的一致性问题，因为数据的同步有一个时间差，并发的写入可能导致数据同步失败
* 相同步长避免冲突：1）设置不同的初始值；2）设置相同的增长步长
* 上游生成ID避冲突：业务上游，使用统一的ID生成器，来保证ID的生成不冲突
* 只使用一个主库提供服务，另一个主库作为shadow-master，只用来保证高可用
  - 缺点：极限的情况下，也可能引发数据的不一致
* 内网DNS探测：使用内网域名连接数据库- 在主库1出现问题后，延时一个时间，再进行主库切换，以保证数据一致性  
  
  
#### 分布式ID生成器
* 问题：如何高效生成趋势有序的全局唯一ID
* 常见方法
  - 方法一：使用数据库的 auto_increment 来生成全局唯一递增ID
    - 优点：1）简单；2）唯一性；3）递增性；4）步长固定
    - 缺点：1）可用性难保证；2）单点扩展性差
    - 改进：1）冗余主库，避免单点写入；2）水平切分数据库
  - 方法二：单点批量ID生成服务，批量发放ID（一次多个）减少压力
    - 优点：1）保证了ID生成的绝对递增有序；2）降低了数据库的压力，ID生成可以做到每秒生成几万几十万个
    - 缺点：1）服务仍然是单点；2）重启服务生成ID可能会不连续；3）无法进行水平扩展
    - 改进：虚拟IP影子备用服务
  - uuid/guid：string ID =GenUUID();
    - 优点：1）本地生成ID，不需要进行远程调用，时延低；2）扩展性好
    - 缺点：1）无法保证趋势递增；2）uuid过长，往往用字符串表示，作为主键建立索引查询效率低
  - 取当前毫秒数：uint64 ID = GenTimeMS();  
    - 优点：1）本地生成ID，时延低；2）趋势递增；3）ID是整数，建立索引后查询效率高
    - 缺点：如果并发量超过1000，会生成重复的ID
  - 类snowflake算法：
    - 优点：1）毫秒数放在最高位，保证生成的ID是趋势递增的；
    - 缺点：由于“没有一个全局时钟”，每台服务器分配的ID是绝对递增的，但从全局看，生成的ID只是趋势递增的
  
  
#### 库存扣多了，到底怎么整
* 容错机制重试，可能导致库存多扣情况
  - 方案一：减扣操作改为设置操作
    - 缺点：并发量大时会覆盖前面的操作
  - 方案二：CAS（Compare And Set）- update stock set num=$num_new where sid=$sid and num=$num_old
  - 方案三：版本号对比
* 总结
  - 调用“设置库存”接口，能够保证数据的幂等性
  - 在实现“设置库存”接口时，需要加上原有库存的比较，才允许设置成功，能解决高并发下库存扣减的一致性问题
  
  
#### 100亿数据1万属性数据架构设计
* 数据库扩展的version + ext方案：使用ext来承载不同业务需求的个性化属性，使用version来标识ext里各个字段的含义
  - 例子：verion=0表示ext里是passwd/nick，version=1表示ext里是passwd/nick/age/sex
  - 优点：1）扩展性好；2）新旧数据共存，兼容性好
  - 缺点：1）ext无法建索引；2）ext里key大量冗余，建议短一些
* 58同城的帖子信息
  - 每个品类的属性千差万别，近万个属性
  - 量很大，100亿级别
  - 每个属性上都有查询需求（各组合属性上都可能有组合查询需求
  - 查询量很大，每秒几10万级别
  - 难题：100亿数据量，1万属性，多属性组合查询，10万并发查询的技术难题
* 方案一：添加字段 + 组合索引 （行不通）
* 方案二：拆分表（问题太多）
* 方案三：
  - 通用的字段单独存储
  - 通过cate, subcate, xxid等来定义ext是何种含义
  - 新问题：1）ext占用大量空间；2）cateid不够描述时，ext能否自描述；3）随时加属性，保证扩展
    - 解决方案：
      - 1）ext里的key用数字表示，抽象出一个类目属性表；value也一样
      - 2）统一检索：所有非“帖子id”的个性化检索需求，统一走外置索引  
  
#### 业界难题-“跨库分页”的四种方案
* 数据量不大时，利用SQL提供的offset/limit功能就能满足分页查询需求
* 数据量变大，水平切分后如何实现分页
  - 方案一：全局视野法 - 每个库都取出N页数据，在内存进行排序，再取出N页数据
    - 优点：业务无损，精准返回数据
    - 缺点：1）返回更多数据，消耗更多网络流量；2）增加CPU计算量；3）页码增加，性能急剧下降
  - 方案二：业务折衷法
    - 折衷1：禁止跳页查询，只提供下一页 - 数据的传输量和排序的数据量不会随着不断翻页而导致性能下降
    - 折衷2：允许数据精度损失 - 在各分库查询一部分数据，然后合并
  - 方案三：终极武器-二次查询法
    - 1）将order by time offset X limit Y，改写成order by time offset X/N limit Y
    - 2）找到最小值time_min
    - 3）between二次查询，order by time between $time_min and $time_i_max
    - 4）设置虚拟time_min，找到time_min在各个分库的offset，从而得到time_min在全局的offset
    - 5）得到了time_min在全局的offset，自然得到了全局的offset X limit Y

  
  
  
### 资料链接
* [架构师之路精选](https://mp.weixin.qq.com/s/CIPosICgva9haqstMDIHag)